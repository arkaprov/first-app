Setting hostname - sudo hostnamectl set-hostname <new_hostname>
Github push repo to github


1. Yum install git -y
2. ssh-keygen
3. Cd .ssh
4. Copy the public key, now add this in github
5. Cd data/
6. git init
7. git add .
8. git commit -m "Initial commit"
9. git remote add origin git@github.com:arkaprovolti/first-app.git
10. git remote -v
11. git push  origin master
12. 

Github: cloning same repo and pushing it to same repo;
1. git clone git@github.com:arkaprovolti/first-app.git
2. cat > sds.txt
sdaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa yeyyy
3. git add .
4.  git commit -m "Initial commit"
5. git push  origin main




Github- cloning a different repo and pushing it to my repo
1. git clone git@github.com:sanjayguruji/java-code-with-maven.git
2. Now go inside /java-code-with-maven directory cloned
3. git remote -v
4. git remote set-url origin git@github.com:arkaprovolti/java-maven-arka-2.git
5. Git push origin main


Jenkins
1. 15 gb gp2 instance
2.  yum update -y
3. Google search installing jenkins on aws
4. sudo wget -O /etc/yum.repos.d/jenkins.repo \
https://pkg.jenkins.io/redhat-stable/jenkins.repo
5. Yum repolist all
6. sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
sudo yum upgrade
7. Instance -> security groups -> select your one -> edit inbound rules ->add custom tcp -> port range 8080 -> 0.0.0.0/0
8. Now copy public ip : 8080
9. sudo yum install java-17-amazon-corretto -y
10. Sudo yum install jenkins -y
11. Systemctl enable jenkins
12. Systemctl start jenkins
13. Open public ip of server
14. Paste vim  /var/lib/jenkins/secrets/initialAdminPassword and unlock jenkins
15. Now gotta increase size of temp
16. sudo mount -o remount, size=2G /tmp
17. df -h
18. vim /etc/fstab
19. tmpfs /tmp tmpfs defaults,noatime,mode=1777,size=2G 0 0
20. Sudo mount -a
21. Systemctl daemon-reload
22. Reboot instance
23. in case if ec2 instance is shutdown, jenkins node will go slow. To solve this, go to
Cd /var/lib/jenkins -> jenkins.model…….xml -> open it in vim -> we got an ip address, replace it with port ip.
24. Now we gotta integrate jenkins server with our github repo.
25. Add webhooks in our github repo
26. Payload url should look like http://51.20.118.218:8080/github-webhook/
27. You gotta get secret, for this go to jenkins -> security -> create api token, githooks -> allow on agents.
28. Now we will make a pipeline in jenkins -> new item -> freestyle ->ok
29. Go to configure, of the created pipeline, and paste repo url in htpps
30. yum install git
31. Now build pipeline


Tomcat server setup
   1. yum install java -y
yum install wget
wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.108/bin/apache-tomcat-9.0.108.tar.gz
tar -xvzf apache-tomcat-10.1.44.tar.gz
mv apache-tomcat-10.1.44.tar.gz /tmp
 cd apache-tomcat-10.1.44
cd bin/
chmod +x startup.sh
chmod +x shutdown.sh
cd ..
cd apache-tomcat-10.1.44
find -name context.xml -> 3 files be created -> now we gotta last 3 run each of em in vim
vim ./webapps/examples/META-INF/context.xml
cd conf
vim tomcat-users.xml
push given lines between --> and </tomcat-users>: 
    <role rolename="manager-gui"/>
<role rolename="manager-script"/>
<role rolename="manager-jmx"/>
<role rolename="manager-status"/>   
<user username="admin" password="admin" roles="manager-gui,manager-script,manager-jmx,manager-status"/>
<user username="deployer" password="deployer" roles="manager-script"/>
<user username="tomcat" password="s3cret" roles="manager-gui"/>

cd bin
./startup.sh

Instance -> security groups -> select your one -> edit inbound rules ->add custom tcp -> port range 8080 -> 0.0.0.0/0
Now copy public ip : 8080





Integrating java GitHub repo with Jenkins (after Jenkins server is setup)

yum install git -y
yum install maven -y
mvn -v  --> can see maven hom path and java path
Now you gotta install plugins - maven integration, deploy to container, GitHub integration
Now restart Jenkins
Now you gotta path set -> Manage Jenkins -> Tools -> add jdk ->java path -> maven- maven path
Now we gotta integrate jenkins server with our github repo.
26. Payload url should look like http://51.20.118.218:8080/github-webhook/
27. You gotta get secret, for this go to jenkins -> security -> create api token, githooks -> allow on agents.
Now new build -> maven project -> git repo https paste -> untick snapshot -> apply and save












Launching index.html on httpd server

1. Launch an ec2 instance
yum install httpd -y
rpmquery httpd
cd /var/www/html
 cat > index.html
give port no.80 
systemctl start httpd
systemctl enable httpd
http://54.235.42.184/:80


Making AMI of instance
1. go to your instance
actions -> image ->create image.
the ami will be created in ami catalog -> my ami
Now launch instance with AMI
Create instance
You will see the same machine content will be copied into a different machine
Now you want to share this AMI for a different account, so go to AMI -> Shared account id ->add account id
Now that account id can see your AMI into his, he can make instance of that and the files will be shared.









Creating a template and launching 2 instances

Ec2->Launch templates -> Create new template -> My AMIS -> owned by me ->  EBS Volume 10gb gp2
Go to a instance -> Launch instance from template
Now I want script automatically triggered -> Advanced Settings -> User Data ->
	yum install httpd -y
	cat > index.html
	systemctl start httpd
	systemctl enable httpd
	useradd John
	passwd John
create launch template
Now, go to an instance and launch instance from template -> give no. of instances 2 -> launch instance
Now we can see 2 instances loaded for the given templat -> You can see if public ip is pasted with port 80 (make sure to configure port 80 in inbound rules)

We can modify template by launching template -> Modify template





Creating Volume in an instance

1. lsblk
2. Now we will create 5 gb volume -> EC2 -> volume -> 5GB
3. Now tick the created volume -> actions -> attach volume -> attach our created instance -> give a device ->
4. lsblk -> we can see our new volume now
5. Now how to access our new disk??
6. lsblk -fs -> we can't see our 5 gb created volume.
7. blkid -> This will show our UUID
8. mkdir /data
9. mount /dev/xvdb /data
df -h
cd /data/
touch Sanjaya.txt{1....100} ->100 blank files created
ll
Now temporary mounting is done. For permanent mounting, we have to follow next steps.
You can get uiid from blkid
vim /etc/fstab -> write
	/dev/xvdb	/data	ext4	defaults 0 0
Restart instance


Extending root volume of an instance

1. Launch an instance
2. create an httpd server
yum install httpd -y
rpmquery httpd
cd /var/www/html
 cat > index.html
give port no.80 
systemctl start httpd
systemctl enable httpd
curl http://localhost -> now the server is live
df -h -> we can see /dev/xvda1 is 10 Gb, which isn't enough for us en, we gotta increase its volume
Now go to volume -> select the volume of the created instance -> modify volume
Now change size to 20 gb
df -h -> but still 10 gb is visible in /dev/xdva1 because OS can read only that storage which have filesystem
So google how to extend xfs filesystem in rhel 8 in aws ec2
growpart /dev/xvda1
growpart /dev/xvda 1
sudo xfs_growfs -d /
df -h -> Now /dev/xvda1 size is increased to 20 gb





Creating a new volume and attaching it with our instance, and also make 2 partitions of this new volume

Create new volume - 7gb, attach it with our instance
fdisk -l
Now, for partitioning we'll use MS DOS Partition scheme
lsblk
fdisk /dev/xvdb
m
n
e
4
 give both sector empty
p
lsblk -> xvdb5 and xvdb6 created we can see
fdisk -l
Now we gotta create mount point. We can use either ext4 or xfs filesystem for xvdb5 and xvdb6 partition.
Now create 2 directories, and mount both in these directories.
mkdir /devops
mkdir /data
mount /dev/xvdb5 /data/
mount /dev/xvdb6 /devops/
cd /devops
touch arka.txt{1...5}

Now if we wanna unmount them and detach them from cloud, and then delete the volume
umount /devops
umount /data





Data replicate in ec2 into different region same zone

1. HW given ->  Data replicate in ec2 into same region different zone
For this we have to use snapshot. It inherits all zones of our region. Snapshot is stored in AWS S3 in backend, but not reachable for us.
2. Our Volume -> create snapshot -> create snapshot
3. Now choose different region and go to snapshot (Ohio - us-east-1b) -> not reachable. Because Snapshot is a region based service. Now copy snapshot and choose us-east-2.




Data replicate in ec2 into same region different zone

For this we have to use snapshot. It inherits all zones of our region. Snapshot is stored in AWS S3 in backend, but not reachable for us.
2. Our Volume -> create snapshot -> create snapshot
3. Now for the created snapshot -> we create volume from snapshot and choose a different region.
Now create an instance for that volume


Data replication in EFS by 3 different machines in 3 different zones in same region

1. Create 3 instances 8 gb each (amazon Linux, redhat, ubuntu) in 3 different regions
2. Same security group with all 3 machines
3. Search EFS -> create file system 
4. For the file system created -> view ->edit
We gotta install nfs in all of 3 machines (nfs is by default installed in amazon Linux)
amazon machine -> rpmquery nfs-utils
		systemctl start nfs-server.service
		systemctl enable nfs-server.service
		systemctl status
redhat -> 
	yum install nfs-utils -y
	systemctl start nfs-server.service
		systemctl enable nfs-server.service
		systemctl status

ubuntu ->
	apt update -y
	apt install nfs-common
	systemctl start nfs-utils.service
	systemctl enable nfs-utils.service
we went to security group and added nfs.
Now go EFS -> file systems -> network -> managet -> security group changed to nfs - a,b,c keep ; deleted rest.
Now we go to our filesystem -> Attach -> Mount via DNS -> copy "using the NFS client" ->
sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-034ce5eaefe890aad.efs.us-east-1.amazonaws.com:/ efs

now we create a directory (data) on either machine in root (for easy), paste the copied sudo mount... and replace efs with our directory.


Now we create another directory in another machine, and paste sudo mount ... and replace efs with our directory, create files in there -> touch fk.txt{1..10}

Now, in our previous mounted machine, in the mounted directory, we can find our files created in previous machine.










Creating a S3 bucket and access it from Amazon Linux instance (and also creating files in machine which shall be reflected in s3 bucket)


s3-> Create bucket _> bucket name, ACLs diabled ->unlock ->acknowledge -> create bucket
Now upload some data on bucket
Now you can't access object with object link.
in permission -> ACl was disabled. So gotta give permission for bucket
Go to -> edit object owbership -> ACL enable
Now u can see for each object -> permission -> Edit is enabled
Now you can edit and give permsiion to objects.
But if deletion of object happens by mistake, u can retain it only if bucket versioning is enabled

1. After bucket is created and files are uploaded, create an instance
2. yum update -y
Now we gotta install some plugins. 
 yum install automake fuse fuse-devel gcc-c++ libcurl-devel libxml2-devel make openssl-devel
yum install git
git clone https://github.com/s3fs-fuse/s3fs-fuse.git
now move to s3fs-fuse directory
./autogen.sh
./configure --prefix=/usr --with-openssl
make
make install
which s3fs
Package is installed, but s3 bucket and instance have different ip so how to access em
AWS ->iam-> create user -> attach policies directly -> give full acess policy rights -> create user
Go to security credential -> create access key -> CLI -> download csv
Now go to cli
touch /etc/passwd-s3fs
vim /etc/passwd-s3fs-> give access-key:secret-key
Now change permission of file -
sudo chmod 640 /etc/passwd-s3fs
Now mount
s3fs arka-bucket1231 /mnt -o passwd_file=/etc/passwd-s3fs
df -h -> s3fs
Now goto mount directory 
 cd /mnt/
touch devops.txt{1..5}
This will be reflected in bucket








Creating a S3 bucket and access it from Amazon Linux instance (using AWS CLI)

1. Create S3 bucket
2. Create an amazon Linux instance
yum install unzip -y
rpmquery mount-s3 -> mount-s3 package not installed, so we gotta install that.
yum install wget -y
wget https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm
yum install ./mount-s3.rpm
Now we need authentication for different services (EC2/local machine and S3 bucket ) to communiccate
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscli2.zip"
unzip awscli2.zip
aws configure -> share access key and secret key from csv -> us-east-1 -> o/p= table
sudo ./aws/install
cd .aws
cat config
cat credentials
Now we have to create a mountpoint, so we create a directory.
mkdir /pooja
mount-s3 arka-bucket1231 /pooja/
Now to go ACL -> give read write permissions and authorize.
cd pooja -> You can see your bucket files.


S3 bucket access from windows instance

1. Create t3.medium instance -> windows machine 2019 base 
security group -> rdp protocol allowed for windows
30 gb gp2.
2. Download RDP file -> open -> connect -> Asking for password
3. Get password in the instance only - upload password
4. Go to server manager -> open -> local server -> ie  -> of of enhance
5. Download WIndows Tnt drive -> install trial .
6. Open tnt drive -> add new drive -> add -> any account name -> access key and password from csv file for authentication -> add new account -> save changes
7. S3 bucket searched -> add new drive -> open it -> create new file -> got out files from s3 bucket.




Static website hosting from s3 bucket

1. open s3 bucket (empty objects).
2. Add all files of any web project (html,css,js)
3. Go to properties -> static web hosting -> edit -> enable ->host static website -> index.html
4. Bucket website endpoint url -> click
5. Permission -> ACL ->edit -> Read, write
6. Seelct all objects -> Action -> make it public -> we got our website


















Creating a load balancer (Create 4 instances on different zones of same region, and while one server is down, other servers would work)

1. Create 4 instances on diff zones.
2. httpd server on all 4 serving index.html
Firstly create target group -> instances -> register targets -> add all machines -> create target group (Ensure all machines are added to the target group)

3. We have to create cross zone load balancer to ensure that if 1 zone gets down, not all can go down.
4. For httpd server, we have to create application load balancer
5. Now how would this load balancer distribute load in the network?? Lets see
6. go to load balancer -> internet facing -> zone a,b,c because our servers in a,b,c -> security group 80 port allowed -> start and enable httpd in all 3 machines -> copy dns and paste




















Creating auto scaling

Autoscaling 

1. Create template -> Browse AMI -> select amazon Linux -> give zone 1a -> create
2. view launch templates
select launched instance -> create auto scaling group ->next
Choose instance launch options -> Instance type requirements -> specify instance attributes -> vpcu 1min 2max -> memory 1min 4max -> Include On-Demand base capacity -> 2 -> in availability zones -> 1a,1b,1c -> Balanced Best Efforts ->next
Integrate with other services-> add new load balancer -> internet facing -> no vpc -> next
Configure group size and scaling -> desired capacity =2 -> scaling 2min 6max ->automatic scaling = target cpu scaling -> avg. cpu utilization -> target value=50, instance warmup 180 secs ->  instance custom policy - custom ->next
Create Auto scaling group
Now in instance can see 2 instances running -> one in us east 1a and other in us east 1b
Now in both instances run this command -> yes > /dev/null & -> This runs yes command in the background, continuously 	generating"y" output. This consumes CPU resources until the process is terminated.
Run command - top
The top command hsows cpu usage, memory usage, process list
Remember the process id of both the machines
Now load will be increased and multiple instances will be created to distribute the load
Now, if we kill either machine or both, the load is reduced, and extra machins will be terminated.
Command to kill process -> kill <PID>







Create VPC and then access EC2. 

1. We have to follow the rule VISR
	V -> VPC ->CIDR (19.0.0/60)
	I -> IGW
	S -> Subnet
 	R -> Route Table.
2. For real life analogy, consider floors as subnets, building as VPC, and every room as EC2.
	So, 1 subnet can have many EC2.
3. We will create 2 subents - private and public. Private won't be exposed over internet, eg. Database. Public will be exposed over internet, eg. Web server, 	Mail server.
4. Suppose, for 2 subnets we wanna create, we allot 250 IPS for one, and 150 IPS for the other. Now how will we subnet? For that we can use vlsm calculator.
5. Now go to AWS -> Create VPC -> VPC only ->IPV4 CIDR -> put 10.0.0.0/16 -> no ipv6 cidr -> create
Now we can see 2 VPCs. The first one is the default one and we don't have to mess with it.
6. Now we have to create IGW (Internet gateway)
7. We can already see one IGW. Now we gotta create one. Give name :arka-igw
8. Now we gotta create subnet -> give arka-vpc -> give name public-subnet -> give zone 1a ->ipv4 vpc cidr block: 10.0.0.0/16 -> ipv4 subnet cidr block: 10.0.0.0/24 -> create subnet
9. Again we create another subnet -> give name private-subnet -> we give zone 1b -> ipv4 vpc cidr block: 10.0.0.0/16 -> ipv4 subnet cidr block: 10.0.1.0/24 -> create subnet
10. Now we gotta create EC2. 2 EC2s to create -> one for server and another for database, both instances should be in different zones.  For the server EC2, we gotta turn on httpd server.
11. For instance 1-> give our vpc -> our public subnet -> disable auto assign public ip -> security group ->create security group new ->launch instance
12. For instance 2-> same key as instance 1 -> disable auto assign public ip -> new security grp -> our private subnet -> 
13. Now for instance 1, allow icmp -> 0.0.0./0 on its security group.
Then we can see that for our server, the public ip is not visible. So we go to elastic ip
allocate elastic ip -> for that elastic ip -> actions -> associate elastic ip -> give that instance

14. Our IGW is not connected with VPC. So we gotta attach IGW with VPC.
15. Now we gotta create Route table (Don't mess with default one) and attach with vpc
16. Now edit routes ->add route 0.0.0.0/0 -> target: IGW
17. Now route table needs to associate with subnet -> our created route-table -> edit subnet association -> select public subnet -> save associations
18. The server instance should work .
19. Now for instance 2(db), our private subnet -> allow icmp -> 0.0.0./0 on its security group,  disable auto assign public ip -> security group ->create security group new 
20. Now NAT Gateway needed for db instance .
21. Create NAT Gateway -> subnet- public ->connectivity type - public -> create
22. Now gotta create another route table ->our vpc-> create -> edit route-> add route 0.0.0.0/0 -> target -NAT -> 
Now for our created route table -> edit subnet association -> select private subnet -> save
Now from our public server -> sudo su -
vim key14auga.pem (from our public server)
paste the full key
 chmod 400  key14auga.pem
Now try accessing the database server, it will run.
ping google
IT WORKS



H.W -North Virginia - Ohio data communication using VPC
















