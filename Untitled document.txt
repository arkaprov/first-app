Setting hostname - sudo hostnamectl set-hostname <new_hostname>
Github push repo to github


1. Yum install git -y
2. ssh-keygen
3. Cd .ssh
4. Copy the public key, now add this in github
5. Cd data/
6. git init
7. git add .
8. git commit -m "Initial commit"
9. git remote add origin git@github.com:arkaprovolti/first-app.git
10. git remote -v
11. git push  origin master
12. 

Github: cloning same repo and pushing it to same repo;
1. git clone git@github.com:arkaprovolti/first-app.git
2. cat > sds.txt
sdaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa yeyyy
3. git add .
4.  git commit -m "Initial commit"
5. git push  origin main




Github- cloning a different repo and pushing it to my repo
1. git clone git@github.com:sanjayguruji/java-code-with-maven.git
2. Now go inside /java-code-with-maven directory cloned
3. git remote -v
4. git remote set-url origin git@github.com:arkaprovolti/java-maven-arka-2.git
5. Git push origin main


Jenkins
1. 15 gb gp2 instance
2.  yum update -y
3. Google search installing jenkins on aws
4. sudo wget -O /etc/yum.repos.d/jenkins.repo \
https://pkg.jenkins.io/redhat-stable/jenkins.repo
5. Yum repolist all
6. sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
sudo yum upgrade
7. Instance -> security groups -> select your one -> edit inbound rules ->add custom tcp -> port range 8080 -> 0.0.0.0/0
8. Now copy public ip : 8080
9. sudo yum install java-17-amazon-corretto -y
10. Sudo yum install jenkins -y
11. Systemctl enable jenkins
12. Systemctl start jenkins
13. Open public ip of server
14. Paste vim  /var/lib/jenkins/secrets/initialAdminPassword and unlock jenkins
15. Now gotta increase size of temp
16. sudo mount -o remount, size=2G /tmp
17. df -h
18. vim /etc/fstab
19. tmpfs /tmp tmpfs defaults,noatime,mode=1777,size=2G 0 0
20. Sudo mount -a
21. Systemctl daemon-reload
22. Reboot instance
23. in case if ec2 instance is shutdown, jenkins node will go slow. To solve this, go to
Cd /var/lib/jenkins -> jenkins.model…….xml -> open it in vim -> we got an ip address, replace it with port ip.
24. Now we gotta integrate jenkins server with our github repo.
25. Add webhooks in our github repo
26. Payload url should look like http://51.20.118.218:8080/github-webhook/
27. You gotta get secret, for this go to jenkins -> security -> create api token, githooks -> allow on agents.
28. Now we will make a pipeline in jenkins -> new item -> freestyle ->ok
29. Go to configure, of the created pipeline, and paste repo url in htpps
30. yum install git
31. Now build pipeline


Tomcat server setup
   1. yum install java -y
yum install wget
wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.108/bin/apache-tomcat-9.0.108.tar.gz
tar -xvzf apache-tomcat-10.1.44.tar.gz
mv apache-tomcat-10.1.44.tar.gz /tmp
 cd apache-tomcat-10.1.44
cd bin/
chmod +x startup.sh
chmod +x shutdown.sh
cd ..
cd apache-tomcat-10.1.44
find -name context.xml -> 3 files be created -> now we gotta last 3 run each of em in vim
vim ./webapps/examples/META-INF/context.xml
cd conf
vim tomcat-users.xml
push given lines between --> and </tomcat-users>: 
    <role rolename="manager-gui"/>
<role rolename="manager-script"/>
<role rolename="manager-jmx"/>
<role rolename="manager-status"/>   
<user username="admin" password="admin" roles="manager-gui,manager-script,manager-jmx,manager-status"/>
<user username="deployer" password="deployer" roles="manager-script"/>
<user username="tomcat" password="s3cret" roles="manager-gui"/>

cd bin
./startup.sh

Instance -> security groups -> select your one -> edit inbound rules ->add custom tcp -> port range 8080 -> 0.0.0.0/0
Now copy public ip : 8080





Integrating java GitHub repo with Jenkins (after Jenkins server is setup)

yum install git -y
yum install maven -y
mvn -v  --> can see maven hom path and java path
Now you gotta install plugins - maven integration, deploy to container, GitHub integration
Now restart Jenkins
Now you gotta path set -> Manage Jenkins -> Tools -> add jdk ->java path -> maven- maven path
Now we gotta integrate jenkins server with our github repo.
26. Payload url should look like http://51.20.118.218:8080/github-webhook/
27. You gotta get secret, for this go to jenkins -> security -> create api token, githooks -> allow on agents.
Now new build -> maven project -> git repo https paste -> untick snapshot -> apply and save












Launching index.html on httpd server

1. Launch an ec2 instance
yum install httpd -y
rpmquery httpd
cd /var/www/html
 cat > index.html
give port no.80 
systemctl start httpd
systemctl enable httpd
http://54.235.42.184/:80


Making AMI of instance
1. go to your instance
actions -> image ->create image.
the ami will be created in ami catalog -> my ami
Now launch instance with AMI
Create instance
You will see the same machine content will be copied into a different machine
Now you want to share this AMI for a different account, so go to AMI -> Shared account id ->add account id
Now that account id can see your AMI into his, he can make instance of that and the files will be shared.









Creating a template and launching 2 instances

Ec2->Launch templates -> Create new template -> My AMIS -> owned by me ->  EBS Volume 10gb gp2
Go to a instance -> Launch instance from template
Now I want script automatically triggered -> Advanced Settings -> User Data ->
	yum install httpd -y
	cat > index.html
	systemctl start httpd
	systemctl enable httpd
	useradd John
	passwd John
create launch template
Now, go to an instance and launch instance from template -> give no. of instances 2 -> launch instance
Now we can see 2 instances loaded for the given templat -> You can see if public ip is pasted with port 80 (make sure to configure port 80 in inbound rules)

We can modify template by launching template -> Modify template





Creating Volume in an instance

1. lsblk
2. Now we will create 5 gb volume -> EC2 -> volume -> 5GB
3. Now tick the created volume -> actions -> attach volume -> attach our created instance -> give a device ->
4. lsblk -> we can see our new volume now
5. Now how to access our new disk??
6. lsblk -fs -> we can't see our 5 gb created volume.
7. blkid -> This will show our UUID
8. mkdir /data
9. mount /dev/xvdb /data
df -h
cd /data/
touch Sanjaya.txt{1....100} ->100 blank files created
ll
Now temporary mounting is done. For permanent mounting, we have to follow next steps.
You can get uiid from blkid
vim /etc/fstab -> write
	/dev/xvdb	/data	ext4	defaults 0 0
Restart instance


Extending root volume of an instance

1. Launch an instance
2. create an httpd server
yum install httpd -y
rpmquery httpd
cd /var/www/html
 cat > index.html
give port no.80 
systemctl start httpd
systemctl enable httpd
curl http://localhost -> now the server is live
df -h -> we can see /dev/xvda1 is 10 Gb, which isn't enough for us en, we gotta increase its volume
Now go to volume -> select the volume of the created instance -> modify volume
Now change size to 20 gb
df -h -> but still 10 gb is visible in /dev/xdva1 because OS can read only that storage which have filesystem
So google how to extend xfs filesystem in rhel 8 in aws ec2
growpart /dev/xvda1
growpart /dev/xvda 1
sudo xfs_growfs -d /
df -h -> Now /dev/xvda1 size is increased to 20 gb





Creating a new volume and attaching it with our instance, and also make 2 partitions of this new volume

Create new volume - 7gb, attach it with our instance
fdisk -l
Now, for partitioning we'll use MS DOS Partition scheme
lsblk
fdisk /dev/xvdb
m
n
e
4
 give both sector empty
p
lsblk -> xvdb5 and xvdb6 created we can see
fdisk -l
Now we gotta create mount point. We can use either ext4 or xfs filesystem for xvdb5 and xvdb6 partition.
Now create 2 directories, and mount both in these directories.
mkdir /devops
mkdir /data
mount /dev/xvdb5 /data/
mount /dev/xvdb6 /devops/
cd /devops
touch arka.txt{1...5}

Now if we wanna unmount them and detach them from cloud, and then delete the volume
umount /devops
umount /data





Data replicate in ec2 into different region same zone

1. HW given ->  Data replicate in ec2 into same region different zone
For this we have to use snapshot. It inherits all zones of our region. Snapshot is stored in AWS S3 in backend, but not reachable for us.
2. Our Volume -> create snapshot -> create snapshot
3. Now choose different region and go to snapshot (Ohio - us-east-1b) -> not reachable. Because Snapshot is a region based service. Now copy snapshot and choose us-east-2.




Data replicate in ec2 into same region different zone

For this we have to use snapshot. It inherits all zones of our region. Snapshot is stored in AWS S3 in backend, but not reachable for us.
2. Our Volume -> create snapshot -> create snapshot
3. Now for the created snapshot -> we create volume from snapshot and choose a different region.
Now create an instance for that volume


Data replication in EFS by 3 different machines in 3 different zones in same region

1. Create 3 instances 8 gb each (amazon Linux, redhat, ubuntu) in 3 different regions
2. Same security group with all 3 machines
3. Search EFS -> create file system 
4. For the file system created -> view ->edit
We gotta install nfs in all of 3 machines (nfs is by default installed in amazon Linux)
amazon machine -> rpmquery nfs-utils
		systemctl start nfs-server.service
		systemctl enable nfs-server.service
		systemctl status
redhat -> 
	yum install nfs-utils -y
	systemctl start nfs-server.service
		systemctl enable nfs-server.service
		systemctl status

ubuntu ->
	apt update -y
	apt install nfs-common
	systemctl start nfs-utils.service
	systemctl enable nfs-utils.service
we went to security group and added nfs.
Now go EFS -> file systems -> network -> managet -> security group changed to nfs - a,b,c keep ; deleted rest.
Now we go to our filesystem -> Attach -> Mount via DNS -> copy "using the NFS client" ->
sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-034ce5eaefe890aad.efs.us-east-1.amazonaws.com:/ efs

now we create a directory (data) on either machine in root (for easy), paste the copied sudo mount... and replace efs with our directory.


Now we create another directory in another machine, and paste sudo mount ... and replace efs with our directory, create files in there -> touch fk.txt{1..10}

Now, in our previous mounted machine, in the mounted directory, we can find our files created in previous machine.










Creating auto scaling





The Plan
We will replace your 4 manually-managed EC2 instances with an Auto Scaling Group (ASG) that automatically manages a fleet of identical instances.

Here are the components we'll create:

AMI (Amazon Machine Image): This will be a "snapshot" or template of your perfectly configured EC2 instance. The ASG will use this to launch new, identical servers.

Launch Template: This is a blueprint that tells the ASG how to launch new instances (which AMI to use, what instance type, which security group, etc.).

Auto Scaling Group (ASG): This is the manager. It will monitor your instances and automatically launch new ones (scale-out) when traffic is high or terminate them (scale-in) when traffic is low.

Step-by-Step Process in the AWS Console
Step 1: Create a "Golden Image" (AMI)
First, we need to create a template from one of your existing, working instances.

Navigate to the EC2 Dashboard in the AWS Console.

In the left menu, click on Instances.

Select one of your 4 running instances that is fully configured with your application and all necessary software.

Click the Actions button at the top right, then navigate to Image and templates -> Create image.

On the "Create image" page:

Image name: Give it a clear, versioned name, like my-app-server-v1.0.

Image description: Add a helpful description, such as Web server for my application, includes Node.js and all dependencies.

No reboot: For the best image quality, it's recommended to let AWS reboot the instance. If you absolutely cannot have a brief moment of downtime on this specific instance, you can check Enable - No reboot, but the standard practice is to allow it.

Click Create image.

You can monitor its progress by going to AMIs in the left navigation pane. Wait for the status to change from pending to available.

Step 2: Create a Launch Template
Now we'll create the blueprint that the Auto Scaling Group will use.

In the EC2 Dashboard's left menu, under "Instances", click on Launch Templates.

Click the Create launch template button.

Fill out the form:

Launch template name: Give it a descriptive name, like my-app-launch-template-v1.

Template version description: Something like Initial template for the main web application.

Auto Scaling guidance: It's helpful to check the box "Provide guidance to help me set up a template for use with EC2 Auto Scaling".

Application and OS Images (Amazon Machine Image): In the search bar, select My AMIs from the dropdown. Then, choose the AMI you created in Step 1 (e.g., my-app-server-v1.0).

Instance type: Select the same instance type as your current machines (e.g., t2.micro, t3.small).

Key pair (login): Select the .pem key pair you use to SSH into your existing instances.

Networking settings:

Leave Subnet as "Don't include in launch template". The Auto Scaling Group will handle this.

Security groups: This is critical. Click "Select existing security group" and choose the same security group your current 4 instances use. This ensures new instances can receive traffic from the Load Balancer.

Click Create launch template.

Step 3: Create the Auto Scaling Group
This is the final step where everything comes together.

In the left menu of the EC2 Dashboard, scroll down to the bottom and click on Auto Scaling Groups (under "Auto Scaling").

Click Create Auto Scaling group.

Wizard - Step 1: Choose launch template or configuration

Auto Scaling group name: my-application-asg

Launch template: Select the launch template you just created (my-app-launch-template-v1). The details should auto-populate.

Click Next.

Wizard - Step 2: Choose instance launch options

Network (VPC): Select the VPC your current instances are in.

Availability Zones and subnets: This is very important. Select all the subnets that your Load Balancer is configured to send traffic to. This allows the ASG to launch instances across different Availability Zones for high availability.

Click Next.

Wizard - Step 3: Configure advanced options

Load balancing: Select Attach to an existing load balancer.

Choose the option Choose from your load balancer target groups.

From the dropdown, select the exact target group that your Load Balancer is currently using.

Health Checks: Check the box for Turn on Elastic Load Balancing (ELB) health checks. This tells the ASG to trust the Load Balancer's health report, which is more reliable than a simple instance check.

Health check grace period: Set this to 300 seconds. This gives new instances 5 minutes to fully boot up and start your application before the ASG starts monitoring their health.

Click Next.

Wizard - Step 4: Configure group size and scaling policies

Group Size:

Desired capacity: The number of instances you want running normally. Let's set it to 2 to start.

Minimum capacity: The absolute minimum number of instances. Set to 1.

Maximum capacity: The maximum number of instances it can scale up to. Let's set it to 6.

Scaling Policies: This is the "auto" part.

Select Target tracking scaling policy.

Metric type: Choose Average CPU utilization.

Target value: Set it to 50. This creates a simple, effective rule:

If average CPU across all instances goes above 50%, the ASG will add new instances.

If average CPU is below 50%, the ASG will remove instances (down to your minimum).

Click Next through the remaining steps (Add notifications, Add tags), reviewing the configuration on the final page.

Wizard - Step 5: Review

Look over all your settings to ensure they are correct.

Click Create Auto Scaling group.

Step 4: Verify and Clean Up
Verify:

Go to the Auto Scaling Groups page. You will see your new ASG initializing.

Go to the EC2 Instances page. You will see 2 new instances being launched by the ASG.

Go to your Target Group page. After a few minutes, you'll see these 2 new instances get registered and their status should eventually become healthy.

Clean Up (Crucial Step):

Once your new ASG-managed instances are healthy in the target group, your application is being served by the new, automated setup.

To avoid paying for double the servers, you must now terminate the original 4 EC2 instances that you created manually. Select them on the Instances page and choose Instance state -> Terminate instance.

You are all set! Your infrastructure is now automated. It will maintain the desired number of instances, replace any that fail, and scale automatically based on CPU load.

Google Privacy PolicyOpens in a new window
Google Terms of ServiceOpens in a new window
Your privacy & Gemini AppsOpens in a new window
Gemini may display inaccurate info, including about people, so double-check its responses.










