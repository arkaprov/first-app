Setting hostname - sudo hostnamectl set-hostname <new_hostname>
Github push repo to github


1. Yum install git -y
2. ssh-keygen
3. Cd .ssh
4. Copy the public key, now add this in github
5. Cd data/
6. git init
7. git add .
8. git commit -m "Initial commit"
9. git remote add origin git@github.com:arkaprovolti/first-app.git
10. git remote -v
11. git push  origin master
12. 

Github: cloning same repo and pushing it to same repo;
1. git clone git@github.com:arkaprovolti/first-app.git
2. cat > sds.txt
sdaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa yeyyy
3. git add .
4.  git commit -m "Initial commit"
5. git push  origin main




Github- cloning a different repo and pushing it to my repo
1. git clone git@github.com:sanjayguruji/java-code-with-maven.git
2. Now go inside /java-code-with-maven directory cloned
3. git remote -v
4. git remote set-url origin git@github.com:arkaprovolti/java-maven-arka-2.git
5. Git push origin main


Jenkins
1. 15 gb gp2 instance
2.  yum update -y
3. Google search installing jenkins on aws
4. sudo wget -O /etc/yum.repos.d/jenkins.repo \
https://pkg.jenkins.io/redhat-stable/jenkins.repo
5. Yum repolist all
6. sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
sudo yum upgrade
7. Instance -> security groups -> select your one -> edit inbound rules ->add custom tcp -> port range 8080 -> 0.0.0.0/0
8. Now copy public ip : 8080
9. sudo yum install java-17-amazon-corretto -y
10. Sudo yum install jenkins -y
11. Systemctl enable jenkins
12. Systemctl start jenkins
13. Open public ip of server
14. Paste vim  /var/lib/jenkins/secrets/initialAdminPassword and unlock jenkins
15. Now gotta increase size of temp
16. sudo mount -o remount, size=2G /tmp
17. df -h
18. vim /etc/fstab
19. tmpfs /tmp tmpfs defaults,noatime,mode=1777,size=2G 0 0
20. Sudo mount -a
21. Systemctl daemon-reload
22. Reboot instance
23. in case if ec2 instance is shutdown, jenkins node will go slow. To solve this, go to
Cd /var/lib/jenkins -> jenkins.model…….xml -> open it in vim -> we got an ip address, replace it with port ip.
24. Now we gotta integrate jenkins server with our github repo.
25. Add webhooks in our github repo
26. Payload url should look like http://51.20.118.218:8080/github-webhook/
27. You gotta get secret, for this go to jenkins -> security -> create api token, githooks -> allow on agents.
28. Now we will make a pipeline in jenkins -> new item -> freestyle ->ok
29. Go to configure, of the created pipeline, and paste repo url in htpps
30. yum install git
31. Now build pipeline


Tomcat server setup
   1. yum install java -y
yum install wget
wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.108/bin/apache-tomcat-9.0.108.tar.gz
tar -xvzf apache-tomcat-10.1.44.tar.gz
mv apache-tomcat-10.1.44.tar.gz /tmp
 cd apache-tomcat-10.1.44
cd bin/
chmod +x startup.sh
chmod +x shutdown.sh
cd ..
cd apache-tomcat-10.1.44
find -name context.xml -> 3 files be created -> now we gotta last 3 run each of em in vim
vim ./webapps/examples/META-INF/context.xml
cd conf
vim tomcat-users.xml
push given lines between --> and </tomcat-users>: 
    <role rolename="manager-gui"/>
<role rolename="manager-script"/>
<role rolename="manager-jmx"/>
<role rolename="manager-status"/>   
<user username="admin" password="admin" roles="manager-gui,manager-script,manager-jmx,manager-status"/>
<user username="deployer" password="deployer" roles="manager-script"/>
<user username="tomcat" password="s3cret" roles="manager-gui"/>

cd bin
./startup.sh

Instance -> security groups -> select your one -> edit inbound rules ->add custom tcp -> port range 8080 -> 0.0.0.0/0
Now copy public ip : 8080





Integrating java GitHub repo with Jenkins (after Jenkins server is setup)

yum install git -y
yum install maven -y
mvn -v  --> can see maven hom path and java path
Now you gotta install plugins - maven integration, deploy to container, GitHub integration
Now restart Jenkins
Now you gotta path set -> Manage Jenkins -> Tools -> add jdk ->java path -> maven- maven path
Now we gotta integrate jenkins server with our github repo.
26. Payload url should look like http://51.20.118.218:8080/github-webhook/
27. You gotta get secret, for this go to jenkins -> security -> create api token, githooks -> allow on agents.
Now new build -> maven project -> git repo https paste -> untick snapshot -> apply and save












Launching index.html on httpd server

1. Launch an ec2 instance
yum install httpd -y
rpmquery httpd
cd /var/www/html
 cat > index.html
give port no.80 
systemctl start httpd
systemctl enable httpd
http://54.235.42.184/:80


Making AMI of instance
1. go to your instance
actions -> image ->create image.
the ami will be created in ami catalog -> my ami
Now launch instance with AMI
Create instance
You will see the same machine content will be copied into a different machine
Now you want to share this AMI for a different account, so go to AMI -> Shared account id ->add account id
Now that account id can see your AMI into his, he can make instance of that and the files will be shared.









Creating a template and launching 2 instances

Ec2->Launch templates -> Create new template -> My AMIS -> owned by me ->  EBS Volume 10gb gp2
Go to a instance -> Launch instance from template
Now I want script automatically triggered -> Advanced Settings -> User Data ->
	yum install httpd -y
	cat > index.html
	systemctl start httpd
	systemctl enable httpd
	useradd John
	passwd John
create launch template
Now, go to an instance and launch instance from template -> give no. of instances 2 -> launch instance
Now we can see 2 instances loaded for the given templat -> You can see if public ip is pasted with port 80 (make sure to configure port 80 in inbound rules)

We can modify template by launching template -> Modify template





Creating Volume in an instance

1. lsblk
2. Now we will create 5 gb volume -> EC2 -> volume -> 5GB
3. Now tick the created volume -> actions -> attach volume -> attach our created instance -> give a device ->
4. lsblk -> we can see our new volume now
5. Now how to access our new disk??
6. lsblk -fs -> we can't see our 5 gb created volume.
7. blkid -> This will show our UUID
mkfs.ext4 /dev/xvdb
8. mkdir /data
9. mount /dev/xvdb /data
df -h
cd /data/
touch Sanjaya.txt{1....100} ->100 blank files created
ll
Now temporary mounting is done. For permanent mounting, we have to follow next steps.
You can get uiid from blkid
vim /etc/fstab -> write
	/dev/xvdb	/data	ext4	defaults 0 0
Restart instance


Extending root volume of an instance

1. Launch an instance
2. create an httpd server
yum install httpd -y
rpmquery httpd
cd /var/www/html
 cat > index.html
give port no.80 
systemctl start httpd
systemctl enable httpd
curl http://localhost -> now the server is live
df -h -> we can see /dev/xvda1 is 10 Gb, which isn't enough for us en, we gotta increase its volume
Now go to volume -> select the volume of the created instance -> modify volume
Now change size to 20 gb
df -h -> but still 10 gb is visible in /dev/xdva1 because OS can read only that storage which have filesystem
So google how to extend xfs filesystem in rhel 8 in aws ec2
growpart /dev/xvda1
growpart /dev/xvda 1
sudo xfs_growfs -d /
df -h -> Now /dev/xvda1 size is increased to 20 gb





Creating a new volume and attaching it with our instance, and also make 2 partitions of this new volume

Create new volume - 7gb, attach it with our instance
fdisk -l
Now, for partitioning we'll use MS DOS Partition scheme
lsblk
fdisk /dev/xvdb
m
n
e
4
 give both sector empty
p
lsblk -> xvdb5 and xvdb6 created we can see
fdisk -l
Now we gotta create mount point. We can use either ext4 or xfs filesystem for xvdb5 and xvdb6 partition.
Now create 2 directories, and mount both in these directories.
mkdir /devops
mkdir /data
mount /dev/xvdb5 /data/
mount /dev/xvdb6 /devops/
cd /devops
touch arka.txt{1...5}

Now if we wanna unmount them and detach them from cloud, and then delete the volume
umount /devops
umount /data





Data replicate in ec2 into different region same zone

1. HW given ->  Data replicate in ec2 into same region different zone
For this we have to use snapshot. It inherits all zones of our region. Snapshot is stored in AWS S3 in backend, but not reachable for us.
2. Our Volume -> create snapshot -> create snapshot
3. Now choose different region and go to snapshot (Ohio - us-east-1b) -> not reachable. Because Snapshot is a region based service. Now copy snapshot and choose us-east-2.




Data replicate in ec2 into same region different zone

For this we have to use snapshot. It inherits all zones of our region. Snapshot is stored in AWS S3 in backend, but not reachable for us.
2. Our Volume -> create snapshot -> create snapshot
3. Now for the created snapshot -> we create volume from snapshot and choose a different region.
Now create an instance for that volume


Data replication in EFS by 3 different machines in 3 different zones in same region

1. Create 3 instances 8 gb each (amazon Linux, redhat, ubuntu) in 3 different regions
2. Same security group with all 3 machines
3. Search EFS -> create file system 
4. For the file system created -> view ->edit
We gotta install nfs in all of 3 machines (nfs is by default installed in amazon Linux)
amazon machine -> rpmquery nfs-utils
		systemctl start nfs-server.service
		systemctl enable nfs-server.service
		systemctl status
redhat -> 
	yum install nfs-utils -y
	systemctl start nfs-server.service
		systemctl enable nfs-server.service
		systemctl status

ubuntu ->
	apt update -y
	apt install nfs-common
	systemctl start nfs-utils.service
	systemctl enable nfs-utils.service
we went to security group and added nfs.
Now go EFS -> file systems -> network -> managet -> security group changed to nfs - a,b,c keep ; deleted rest.
Now we go to our filesystem -> Attach -> Mount via DNS -> copy "using the NFS client" ->
sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-034ce5eaefe890aad.efs.us-east-1.amazonaws.com:/ efs

now we create a directory (data) on either machine in root (for easy), paste the copied sudo mount... and replace efs with our directory.


Now we create another directory in another machine, and paste sudo mount ... and replace efs with our directory, create files in there -> touch fk.txt{1..10}

Now, in our previous mounted machine, in the mounted directory, we can find our files created in previous machine.










Creating a S3 bucket and access it from Amazon Linux instance (and also creating files in machine which shall be reflected in s3 bucket)


s3-> Create bucket _> bucket name, ACLs diabled ->unlock ->acknowledge -> create bucket
Now upload some data on bucket
Now you can't access object with object link.
in permission -> ACl was disabled. So gotta give permission for bucket
Go to -> edit object owbership -> ACL enable
Now u can see for each object -> permission -> Edit is enabled
Now you can edit and give permsiion to objects.
But if deletion of object happens by mistake, u can retain it only if bucket versioning is enabled

1. After bucket is created and files are uploaded, create an instance
2. yum update -y
Now we gotta install some plugins. 
 yum install automake fuse fuse-devel gcc-c++ libcurl-devel libxml2-devel make openssl-devel
yum install git
git clone https://github.com/s3fs-fuse/s3fs-fuse.git
now move to s3fs-fuse directory
./autogen.sh
./configure --prefix=/usr --with-openssl
make
make install
which s3fs
Package is installed, but s3 bucket and instance have different ip so how to access em
AWS ->iam-> create user -> attach policies directly -> give full acess policy rights -> create user
Go to security credential -> create access key -> CLI -> download csv
Now go to cli
touch /etc/passwd-s3fs
vim /etc/passwd-s3fs-> give access-key:secret-key
Now change permission of file -
sudo chmod 640 /etc/passwd-s3fs
Now mount
s3fs arka-bucket1231 /mnt -o passwd_file=/etc/passwd-s3fs
df -h -> s3fs
Now goto mount directory 
 cd /mnt/
touch devops.txt{1..5}
This will be reflected in bucket








Creating a S3 bucket and access it from Amazon Linux instance (using AWS CLI)

1. Create S3 bucket
2. Create an amazon Linux instance
yum install unzip -y
rpmquery mount-s3 -> mount-s3 package not installed, so we gotta install that.
yum install wget -y
wget https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm
yum install ./mount-s3.rpm
Now we need authentication for different services (EC2/local machine and S3 bucket ) to communiccate
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscli2.zip"
unzip awscli2.zip
aws configure -> share access key and secret key from csv -> us-east-1 -> o/p= table
sudo ./aws/install
cd .aws
cat config
cat credentials
Now we have to create a mountpoint, so we create a directory.
mkdir /pooja
mount-s3 arka-bucket1231 /pooja/
Now to go ACL -> give read write permissions and authorize.
cd pooja -> You can see your bucket files.


S3 bucket access from windows instance

1. Create t3.medium instance -> windows machine 2019 base 
security group -> rdp protocol allowed for windows
30 gb gp2.
2. Download RDP file -> open -> connect -> Asking for password
3. Get password in the instance only - upload password
4. Go to server manager -> open -> local server -> ie  -> of of enhance
5. Download WIndows Tnt drive -> install trial .
6. Open tnt drive -> add new drive -> add -> any account name -> access key and password from csv file for authentication -> add new account -> save changes
7. S3 bucket searched -> add new drive -> open it -> create new file -> got out files from s3 bucket.




Static website hosting from s3 bucket

1. open s3 bucket (empty objects).
2. Add all files of any web project (html,css,js)
3. Go to properties -> static web hosting -> edit -> enable ->host static website -> index.html
4. Bucket website endpoint url -> click
5. Permission -> ACL ->edit -> Read, write
6. Seelct all objects -> Action -> make it public -> we got our website


















Creating a load balancer (Create 4 instances on different zones of same region, and while one server is down, other servers would work)

1. Create 4 instances on diff zones.
2. httpd server on all 4 serving index.html
Firstly create target group -> instances -> register targets -> add all machines -> create target group (Ensure all machines are added to the target group)

3. We have to create cross zone load balancer to ensure that if 1 zone gets down, not all can go down.
4. For httpd server, we have to create application load balancer
5. Now how would this load balancer distribute load in the network?? Lets see
6. go to load balancer -> internet facing -> zone a,b,c because our servers in a,b,c -> security group 80 port allowed -> start and enable httpd in all 3 machines -> copy dns and paste




















Creating auto scaling

Autoscaling 

1. Create template -> Browse AMI -> select amazon Linux -> give zone 1a -> create
2. view launch templates
select launched instance -> create auto scaling group ->next
Choose instance launch options -> Instance type requirements -> specify instance attributes -> vpcu 1min 2max -> memory 1min 4max -> Include On-Demand base capacity -> 2 -> in availability zones -> 1a,1b,1c -> Balanced Best Efforts ->next
Integrate with other services-> add new load balancer -> internet facing -> no vpc -> next
Configure group size and scaling -> desired capacity =2 -> scaling 2min 6max ->automatic scaling = target cpu scaling -> avg. cpu utilization -> target value=50, instance warmup 180 secs ->  instance custom policy - custom ->next
Create Auto scaling group
Now in instance can see 2 instances running -> one in us east 1a and other in us east 1b
Now in both instances run this command -> yes > /dev/null & -> This runs yes command in the background, continuously 	generating"y" output. This consumes CPU resources until the process is terminated.
Run command - top
The top command hsows cpu usage, memory usage, process list
Remember the process id of both the machines
Now load will be increased and multiple instances will be created to distribute the load
Now, if we kill either machine or both, the load is reduced, and extra machins will be terminated.
Command to kill process -> kill <PID>







Create VPC and then access EC2. 

1. We have to follow the rule VISR
	V -> VPC ->CIDR (19.0.0/60)
	I -> IGW
	S -> Subnet
 	R -> Route Table.
2. For real life analogy, consider floors as subnets, building as VPC, and every room as EC2.
	So, 1 subnet can have many EC2.
3. We will create 2 subents - private and public. Private won't be exposed over internet, eg. Database. Public will be exposed over internet, eg. Web server, 	Mail server.
4. Suppose, for 2 subnets we wanna create, we allot 250 IPS for one, and 150 IPS for the other. Now how will we subnet? For that we can use vlsm calculator.
5. Now go to AWS -> Create VPC -> VPC only ->IPV4 CIDR -> put 10.0.0.0/16 -> no ipv6 cidr -> create
Now we can see 2 VPCs. The first one is the default one and we don't have to mess with it.
6. Now we have to create IGW (Internet gateway)
7. We can already see one IGW. Now we gotta create one. Give name :arka-igw
8. Now we gotta create subnet -> give arka-vpc -> give name public-subnet -> give zone 1a ->ipv4 vpc cidr block: 10.0.0.0/16 -> ipv4 subnet cidr block: 10.0.0.0/24 -> create subnet
9. Again we create another subnet -> give name private-subnet -> we give zone 1b -> ipv4 vpc cidr block: 10.0.0.0/16 -> ipv4 subnet cidr block: 10.0.1.0/24 -> create subnet
10. Now we gotta create EC2. 2 EC2s to create -> one for server and another for database, both instances should be in different zones.  For the server EC2, we gotta turn on httpd server.
11. For instance 1-> give our vpc -> our public subnet -> disable auto assign public ip -> security group ->create security group new ->launch instance
12. For instance 2-> same key as instance 1 -> disable auto assign public ip -> new security grp -> our private subnet -> 
13. Now for instance 1, allow icmp -> 0.0.0./0 on its security group.
Then we can see that for our server, the public ip is not visible. So we go to elastic ip
allocate elastic ip -> for that elastic ip -> actions -> associate elastic ip -> give that instance

14. Our IGW is not connected with VPC. So we gotta attach IGW with VPC.
15. Now we gotta create Route table (Don't mess with default one) and attach with vpc
16. Now edit routes ->add route 0.0.0.0/0 -> target: IGW
17. Now route table needs to associate with subnet -> our created route-table -> edit subnet association -> select public subnet -> save associations
18. The server instance should work .
19. Now for instance 2(db), our private subnet -> allow icmp -> 0.0.0./0 on its security group,  disable auto assign public ip -> security group ->create security group new 
20. Now NAT Gateway needed for db instance .
21. Create NAT Gateway -> subnet- public ->connectivity type - public -> create
22. Now gotta create another route table ->our vpc-> create -> edit route-> add route 0.0.0.0/0 -> target -NAT -> 
Now for our created route table -> edit subnet association -> select private subnet -> save
Now from our public server -> sudo su -
vim key14auga.pem (from our public server)
paste the full key
 chmod 400  key14auga.pem
Now try accessing the database server, it will run.
ping google
IT WORKS



H.W -North Virginia - Ohio data communication using VPC








Peering connection VPC between Ohio and North Virginia

1. VISR (for Virginia) vpc- ipv4 10.0.0.0/16, public subnet- 10.0.0.0/24, private subnet - 10.0.1.0/24
 2. VISR (for Virginia) vpc- ipv4 20.0.0.0/16, public subnet- 20.0.0.0/24, private subnet - 20.0.1.0/24
3. instance - auto assign ip enable -> subnet -public
attach both igws (ohio and virgina) to vpc
Now, before peering connection, go to both route tables (ohio virginia) and 0.0.0/0 and select igw.
Now peering connection in both ohio and virginia -> select my account and another region -> select ohio for virgina and ovirginia for ohio
now accept peering from both ohio and virginia
route table ->subnet association ->select public (Do for both ohio and virginia)
Try launching instance at last
After peering connection, in route table, give virginia-ip/16 for ohio and ohio-ip/16 for virginia







Installing docker

	search docker install in rhel 8 in google


yum repolist all
sudo dnf -y install dnf-plugins-core
 sudo dnf config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo
yum repolist all -> now we can see docker- docker ce
sudo dnf install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
sudo dnf install docker-ce*
yum install docker* -y
rpmquery docker
systemctl start docker
systemctl enable docker
docker info

overlay2 -> storage driver
/var/lib/docker -> docker root directory

docker ps -> How many containers running
docker ps -a
docker image ls  -> how many images.
	Now how to get the image? -> DockerHub. No registration needed to pull the image, but registration needed to push the docker image.
	Now, suppose we wanna run ubuntu in container... Then we gotta pull docker image. repo -> collection of images
	Get ubuntu image -> docker pull ubuntu:rolling
	Get httpd image -> docker pull httpd:trixie
	Get centos image -> docker pull centos:centos7.9.2009

So, before deploying container, we get to install these images.
docker image ls -> list of our 3 images installed
docker --help

docker run -it --name <container_name> <image_name> /bin/bash
docker run -it --name arka-devops-container ubuntu:rolling /bin/bash

docker ps -a

apt update -y


docker exec -it 8b8d54855d19 /bin/bash -> get into running docker container (ours is ubuntu container)
apt update -y
apt install apache2 -y
cd /var/www/html
 echo "This is my application one">index.html
systemctl apache2 start -> won't work in our running container
service apache2 start
curl http://localhost -> won't work in our runnin container
apt install curl -> curl is a command line browser
cd
mkdir data
cd data
touch arka.txt{1..10}
exit -> we exited from our container arka-devops-container
docker inspect arka-devops-container | less
ip a s
Now we gonna run another container.....
docker image ls -> ubuntu, httpd, centos

docker run -it --name arka-devops-container2 ubuntu:rolling /bin/bash 
apt update -y
apt install apache2 -y
cd /var/www/html
cat index.html
service apache2 start
Ctrl P + Ctrl Q -> container exit
After container exit, curl http://172.17.0.2 -> This is my application one

docker attach arka-devops-container2 -> get into running docker container

Docker works on port forwarding

Ok, now we want my containers to run on a different port.



docker run -it --name arka-devops-container3 -p 8080:80 ubuntu:rolling /bin/bash --> 8080:80: Maps port 8080 on 	your host to port 80 inside the container.
apt update -y
apt install apache2 -y

docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' arka-devops-container2 -> check ip 	address of this docker container
curl http://172.17.0.3   -> This is my application 2

curl http://172.17.0.4  ->Failed to connect to 172.17.0.4 port 80 after 0 ms: Could not connect to server
			For this port 80, we have to mention port 8080 in the security group










Stopping the existing container - docker stop <container-name or id>
Removing the container - docker rm <container-name or id>
Run a new container with volume mounted at / : docker run -it --name webApp -v my-vol:/ ubuntu:rolling /bin/bash
killing docker container: docker kill <container>
removing container: docker rm <container>


#We cannot directly modify the mount point of an existing container's volume while it is running or stopped,
as Docker does not allow changing the mount configuration after creation



Creating Docker Volume

docker volume create myvol
docker volume ls
docker run -it --name mycontainer -v my-vol:/www/html ubuntu:rolling /bin/bash



Docker volume on container (method 1)
1. docker volume create myvol
2. Access the volume using a temporary container: 
docker run -it --rm -v myvol:/data ubuntu:rolling /bin/bash   -> -v myvol:/data mounts the myvol volume to /data inside the container.
3.ls -l /data
4.echo "test data"> /data/testfile.txt
exit
5. Inspect the volume location on the host (amazon instance)(read only access):
	docker volume inspect myvol
6. Look for Mountpoint field (/var/lib/docker/volumes/myvol/_data).
7. Access it with root priviledges (read only)
	sudo ls -l /var/lib/docker/volumes/myvol/_data
	sudo cat /var/lib/docker/volumes/myvol/_data/testfile.txt




Docker volume on container(method 2):

1. Create files on machine : mkdir -p /home/user/my-files
2. Add some files to it:
	echo "File from machine 1"> /home/user/my-files/machone-file1.txt
	echo "File from machine 2"> /home/user/my-files/machone-file2.txt
3. Run a container with a bind mount
	docker run -d --name myContainer -v /home/user/my-files:/data ubuntu:rolling -> Binds the host directory to /data in the container.
4. Access the files in container and add files :
	docker exec -it myContainer /bin/bash
5. Create new files in /data
	echo "File1 from container"> /home/user/my-files/container-file1.txt
	echo "File2 from container"> /home/user/my-files/container-file2.txt
6. Remove the container
	docker stop file-test
	docker rm file-test -> This deletes the container, but the bind-mounted data remains on the host (/home/user/my-files)
7. Access the Volume data on the machine
	ls -l /home/user/my-file










Configuring KOPS: (details is on Sanjaya sir's repo)

create instance ubuntu 2nd in the list t2.medium 10gb
apt update -y
install AWSCLI -> 
apt install unzip -y
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

Install kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
kubectl version --client


INstall kops
curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x ./kops
sudo mv ./kops /usr/local/bin/


Create an IAM user firstly with AdministratorAccess, Route53full, EC2full, IAMfull and S3full full access -> arka-kops-2
Now create access key -> CLI -> 
Now gotta create role-> Now we create role, after user -> aws service -> ec2 -> Route53full, EC2full, IAMfull and S3full full access -> role name : arka2-kops-role-2 -> go to instance and modify iam role

Now, aws configure

Now, Install kops on ubuntu instance:
 curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64

 chmod +x kops-linux-amd64
 sudo mv kops-linux-amd64 /usr/local/bin/kops
kops version 

Create a Route53 private hosted zone (you can create Public hosted zone if you have a domain) ->
Route 53 ->Hosted zones -> create hosted zones -> now while giving name give it in Sanjaya sir's format to be on the safe side -> name: k8s.arka-2-melo.in -> private hosted zone ->

create an S3 bucket: (To be on safe side, give name like Sanjaya Sir's one (dev.k8....)(Give dns name same as bucket)
aws s3 mb s3://dev.k8s.arka-2-melo.in -> make_bucket: dev.k8s.arka-2-melo.in

To Check Bucket: aws s3 ls


Expose environment variable:

 export KOPS_STATE_STORE=s3://dev.k8s.arka-2-melo.in
Create sshkeys before creating cluster

 ssh-keygen

Create kubernetes cluster definitions on S3 bucket: 

kops create cluster --cloud=aws --zones=us-east-1a --name=dev.k8s.arka-2-melo.in --dns-zone=k8s.arka-2-melo.in --dns private

Create kubernetes cluser

  kops update cluster dev.k8s.arka-2-melo.in --yes --admin


Now worker node and master node will be created in instance, wait for 10 mins, they will be ready.

To list nodes
	kubectl get nodes 

Validate your cluster
	kops validate cluster










Kubernetes cluster configuration in master, worker1 and worker 2 (Sanjaya-K8S-Code
/k8s-installtion-with-containerd-on-ubuntu 24.04)



1. ICMP allowed in all 3, with separate for master.
2. t2.medium for master while t2.micro for workers.
3. ip a s -> note down ip address of all 3 machines and at vim.etc/hosts, paste 3 ip with names at all 3.	
	172.31.22.148   master
	172.31.20.241   worker1
	172.31.26.74    worker2
4. try pinging from all 3 to all 3.
5. Now try sir's GitHub repo: k8s-installtion-with-containerd-on-ubuntu 24.04
for master:
	run codes till  sudo chown.
In this command -> vim /etc/containerd/config.toml-> line 139 (:se nu) -> make it true
6. for workers:
	run codes till sudo systemctl enable --now kubelet..
	dont run kubeadm pull and init.
7. then in workers run: 
kubeadm join 172.31.31.6:6443 --token mf0okw.911w0h06l3gffq1a \
        --discovery-token-ca-cert-hash sha256:815d91ea8044ddc548deb6ef3a5890f2ff965b78db4322aaab4de9435a44f89c

8. Then allow ports: 
	port 6443 should be allowed
	for master:
	https://kubernetes.io/docs/reference/networking/ports-and-protocols/

	for workers:
	https://kubernetes.io/docs/reference/networking/ports-and-protocols/













